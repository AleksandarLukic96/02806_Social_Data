{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 - Visualizations\n",
    "\n",
    "By Group XX:\n",
    "\n",
    "-   Aleksandar Lukic - s194066\n",
    "-   Paula Barho - s242926\n",
    "-   Victor Gustav Harbo Rasmussen - s204475\n",
    "\n",
    "As explained in in class during Lecture 1, each week of this class is an Jupyter notebook like this one. In order to follow the class, you simply start reading from the top, following the instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Today's lecture does a few things.\n",
    "* First there is an introduction to data visualization incl a little exercise and a video (Part 1). \n",
    "* As the main event, we will work with crime-data and generate a large number of interesting and informative plots (Part 2,4,5).\n",
    "* We will also talk a bit about what makes a good plot (Part 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: A little visualization exercise\n",
    "\n",
    "Start by downloading these four datasets: [Data 1](https://raw.githubusercontent.com/suneman/socialdata2023/main/files/data1.tsv), [Data 2](https://raw.githubusercontent.com/suneman/socialdata2023/main/files/data2.tsv), [Data 3](https://raw.githubusercontent.com/suneman/socialdata2023/main/files/data3.tsv), and [Data 4](https://raw.githubusercontent.com/suneman/socialdata2023/main/files/data4.tsv). The format is `.tsv`, which stands for _tab separated values_. \n",
    "As you will later realize, these are famous datasets!\n",
    "Each file has two columns (separated using the tab character). The first column is $x$-values, and the second column is $y$-values.  \n",
    "\n",
    "Now, to the exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas display options to show all columns for .head command\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)        # Auto-detect the display width\n",
    "pd.set_option('display.max_colwidth', None) # Show full content of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get path of data directory\n",
    "files_path = os.path.abspath(os.path.join(os.pardir, \"files\"))\n",
    "files_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get path of plot directory\n",
    "plots_path = os.path.abspath(os.path.join(os.pardir, \"plots\"))\n",
    "plots_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create paths to the datasets\n",
    "data_1_path = os.path.join(files_path, \"data1.tsv\")\n",
    "data_2_path = os.path.join(files_path, \"data2.tsv\")\n",
    "data_3_path = os.path.join(files_path, \"data3.tsv\")\n",
    "data_4_path = os.path.join(files_path, \"data4.tsv\") \n",
    "\n",
    "# Load the datasets from the files folder\n",
    "df_1 = pd.read_csv(data_1_path, sep='\\t', header=None)\n",
    "df_2 = pd.read_csv(data_2_path, sep='\\t', header=None)\n",
    "df_3 = pd.read_csv(data_3_path, sep='\\t', header=None)\n",
    "df_4 = pd.read_csv(data_4_path, sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the X and Y columns for each dataset\n",
    "X1 = df_1.iloc[:, 0]\n",
    "Y1 = df_1.iloc[:, 1]\n",
    "X2 = df_2.iloc[:, 0]\n",
    "Y2 = df_2.iloc[:, 1]\n",
    "X3 = df_3.iloc[:, 0]\n",
    "Y3 = df_3.iloc[:, 1]\n",
    "X4 = df_4.iloc[:, 0]\n",
    "Y4 = df_4.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise*: \n",
    "> \n",
    "> * Using the `numpy` function `mean`, calculate the mean of both $x$-values and $y$-values for each dataset. \n",
    ">      * Use python string formatting to print precisely two decimal places of these results to the output cell. Check out [this _stackoverflow_ page](http://stackoverflow.com/questions/8885663/how-to-format-a-floating-number-to-fixed-width-in-python) for help with the string formatting. <font color='grey'>You may also ask an LLM about the string formatting, but make sure it doesn't just give you the answer: Ask how string formatting works, get some examples, and solve your own problem based on that.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean values for each dataset\n",
    "mean_x_1 = np.mean(X1)\n",
    "mean_y_1 = np.mean(Y1)\n",
    "mean_x_2 = np.mean(X2)\n",
    "mean_y_2 = np.mean(Y2)\n",
    "mean_x_3 = np.mean(X3)\n",
    "mean_y_3 = np.mean(Y3)\n",
    "mean_x_4 = np.mean(X4)\n",
    "mean_y_4 = np.mean(Y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formating the mean values to 2 decimal places\n",
    "means = [mean_x_1, mean_y_1, mean_x_2, mean_y_2, mean_x_3, mean_y_3, mean_x_4, mean_y_4]\n",
    "names = [\"x_1\", \"y_1\", \"x_2\", \"y_2\", \"x_3\", \"y_3\", \"x_4\", \"y_4\"]\n",
    "means_rounded = ['%.2f' % mu for mu in means]\n",
    "\n",
    "# Formating with column names and mean values\n",
    "res = zip(names, means_rounded)\n",
    "print(\"MEANS\")\n",
    "print(*res, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Now calculate the variance for all of the various sets of $x$- and $y$-values, by using the `numpy` function `var`. <font color='grey'>You should be able to do this without any help from LLMs.</font> Print it to three decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the variances for each dataset\n",
    "var_x_1 = np.var(X1)\n",
    "var_y_1 = np.var(Y1)\n",
    "var_x_2 = np.var(X2)\n",
    "var_y_2 = np.var(Y2)\n",
    "var_x_3 = np.var(X3)\n",
    "var_y_3 = np.var(Y3)\n",
    "var_x_4 = np.var(X4)\n",
    "var_y_4 = np.var(Y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formating the variances to 2 decimal places\n",
    "vars = [var_x_1, var_y_1, var_x_2, var_y_2, var_x_3, var_y_3, var_x_4, var_y_4]\n",
    "names = [\"x_1\", \"y_1\", \"x_2\", \"y_2\", \"x_3\", \"y_3\", \"x_4\", \"y_4\"]\n",
    "vars_rounded = ['%.3f' % sigma for sigma in vars]\n",
    "\n",
    "# Formating with column names and mean values\n",
    "res = zip(names, vars_rounded)\n",
    "print(\"VARIANCES\")\n",
    "print(*res, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Use `numpy` to calculate the [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient) between $x$- and $y$-values for all four data sets (also print to three decimal places)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Pearson correlation coefficient for each dataset\n",
    "corr_1 = np.corrcoef(X1, Y1)[0][1]\n",
    "corr_2 = np.corrcoef(X2, Y2)[0][1]\n",
    "corr_3 = np.corrcoef(X3, Y3)[0][1]\n",
    "corr_4 = np.corrcoef(X4, Y4)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formating the correlation coefficients to 3 decimal places\n",
    "corrs = [corr_1, corr_2, corr_3, corr_4]\n",
    "corrs_rounded = ['%.3f' % corr for corr in corrs]\n",
    "\n",
    "# Formating with column names and mean values\n",
    "res = zip(\n",
    "    [\"X1\", \"X2\", \"X3\", \"X4\"], \n",
    "    [\"Y1\", \"Y2\", \"Y3\", \"Y4\"], \n",
    "    vars_rounded\n",
    ")\n",
    "print(\"PEARSON CORRELATION COEFFICIENTS\")\n",
    "print(*res, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * The next step is use _linear regression_ to fit a straight line $f(x) = a x + b$ through each dataset and report $a$ and $b$ (to two decimal places). An easy way to fit a straight line in Python is using `scipy`'s `linregress`. It works like this\n",
    "> ```\n",
    "> from scipy import stats\n",
    "> a, b, r_value, p_value, std_err = stats.linregress(x,y)\n",
    ">```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to print the linear regression results\n",
    "def compute_linear_regression(X, Y):\n",
    "    a, b, r_value, p_value, std_err = stats.linregress(X, Y)\n",
    "    print(\"a: \", '%.3f' % a)\n",
    "    print(\"b: \", '%.3f' % b)\n",
    "    print(\"R-squared: \", '%.3f' % r_value**2)\n",
    "    print(\"P-value: \", '%.3f' % p_value)\n",
    "    print(\"Standard error: \", '%.3f' % std_err)\n",
    "    print(\"\\n\")\n",
    "    return a, b, r_value, p_value, std_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_1, b_1, r_value_1, p_value_1, std_err_1 = compute_linear_regression(X1, Y1)\n",
    "a_2, b_2, r_value_2, p_value_2, std_err_2 = compute_linear_regression(X2, Y2)\n",
    "a_3, b_3, r_value_3, p_value_3, std_err_3 = compute_linear_regression(X3, Y3)\n",
    "a_4, b_4, r_value_4, p_value_4, std_err_4 = compute_linear_regression(X4, Y4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Comment on the results from the previous steps. What do you observe? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would seem so far, that the four datasets share very similar statistical properties. This could suggest that the dataset are identical or very similar in their datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Finally, it's time to plot the four datasets using `matplotlib.pyplot`. Use a two-by-two [`subplot`](http://matplotlib.org/examples/pylab_examples/subplot_demo.html) to put all of the plots nicely in a grid and use the same $x$ and $y$ range for all four plots. And include the linear fit in all four plots. (To get a sense of what I think the plot should look like, you can take a look at my version [here](https://raw.githubusercontent.com/suneman/socialdata2023/main/files/anscombe.png).) <font color='grey'>For this sub-exercise, try to write the code based on the example I link to. If you get stuck, you may use your LLM to figure it out. But don't ask it for the solution, ask for help with figuring out how to use the functions!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2x2 grid of subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 7))\n",
    "fig.tight_layout(pad = 5.0)  # Increase padding to avoid overlap\n",
    "\n",
    "# Set the same ranges of x- and y-axis for all subplots\n",
    "xlim = (0, 20)\n",
    "ylim = (0, 15)\n",
    "plt.setp(axs, xlim=xlim, ylim=ylim)\n",
    "\n",
    "# Plot the data points for each dataset\n",
    "axs[0, 0].scatter(X1, Y1, color='black', label='Data 1')\n",
    "axs[0, 0].set_title('Data 1')\n",
    "axs[0, 1].scatter(X2, Y2, color='black', label='Data 2')\n",
    "axs[0, 1].set_title('Data 2')\n",
    "axs[1, 0].scatter(X3, Y3, color='black', label='Data 3')\n",
    "axs[1, 0].set_title('Data 3')\n",
    "axs[1, 1].scatter(X4, Y4, color='black', label='Data 4')\n",
    "axs[1, 1].set_title('Data 4')\n",
    "\n",
    "# Plot the linear regression lines\n",
    "x = np.array(xlim)\n",
    "\n",
    "def fx(x, a, b):\n",
    "    return a*x + b\n",
    "\n",
    "axs[0, 0].plot(x, fx(x, a_1, b_1), color='red')\n",
    "axs[0, 1].plot(x, fx(x, a_2, b_2), color='red')\n",
    "axs[1, 0].plot(x, fx(x, a_3, b_3), color='red')\n",
    "axs[1, 1].plot(x, fx(x, a_4, b_4), color='red')\n",
    "\n",
    "plt.savefig(os.path.join(plots_path, \"W2_ex1_scatterplot.pdf\"), format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Explain - in your own words - what you think my point with this exercise is (see below for tips on this). <font color='grey'>Again, try to write down your own thoughts first. Then you can ask your LLM for help after that</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the plots, the regression for all of the dataset are almost identical. However, this is simply a very intricate coinsidence, as the points are placed in a way which make the linear regression limited in its effectivness. These exact datasets are what makes up [_Anscombe's quartet_](https://en.wikipedia.org/wiki/Anscombe%27s_quartet). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * <font color='grey'>What did you get out of asking the LLM about the previous sub-question? How did you even go about asking the LLM about the point of the entire set of questions? Reflect on whether or not the LLM helped you get smarter?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not use any LLM for this exercise. Simply using google and reading the documentation was enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Excercise:* Questions for this [YouTube Lecture](https://www.youtube.com/watch?v=9D2aI30AMhM&ab_channel=SuneLehmann). \n",
    "> \n",
    "> * What is the difference between *data* and *metadata*? How does that relate to the GPS tracks-example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of data for statistical data analysis, _Data_ are the measurements, numbers, text, etc. which we use to explain stuff. _Metadata_ is the contextual information about the data we are using.\n",
    "\n",
    "_**Data**_: The longitude and latitude decimal values measured on a bike trip.\n",
    "\n",
    "_**Metadata**_: The variables names and comments in the dataset file, which helps us understand what the values represent, e.g. `lon=123.456, lat=123.456`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Sune says that the human eye is a great tool for data analysis. Do you agree? Explain why/why not. Mention something that the human eye is very good at. Can you think of something that [is difficult for the human eye](http://cdn.ebaumsworld.com/mediaFiles/picture/718392/84732652.jpg). Explain why your example is difficult. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sune explains how our eyes collect the information in front of us, and then how we percieve it. He highlights the eye's capibility of seeing the bigger picture, whereas simple statistics might fail. This is clearly demonstrated in the previous execise in this notebook with the Anscombe's quartet. \n",
    "\n",
    "In Sune's example, 1 vs 1000 can be presented visually in different way to convey the meaning. The magnitude of difference between data points can be hard to grasp conceptually, but if we manage to present it more visually aggresively, it can transmit the underlying information more clearly, e.g. the dots and the lines from Sune's examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Simpson's paradox is hard to explain. Come up with your own example - or find one on line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _Simpson's Paradox_ explains how aggregated data may show misleading trends, while its underlying sub-classifications may show more accurately described isolated trends.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * In your own words, explain the differnece between *exploratory* and *explanatory* data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploratory Data Analysis** is when we are looking to test or explore a hypothesis or looking for mmeaning within a dataset.\n",
    "\n",
    "**Explanatory Data Analysis** is when we are aware of trends within a dataset and want to convey this information to others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Visualizing patterns in the data\n",
    "\n",
    "Let's see if we can detect any interesting patterns in the big crime-data file from San Francisco you downloaded last week. We'll again only look at the focus-crimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focuscrimes = set([\n",
    "    'ASSAULT',\n",
    "    'BURGLARY',\n",
    "    'DRUG/NARCOTIC',\n",
    "    'DRUNKENNESS',\n",
    "    'LARCENY/THEFT',\n",
    "    'PROSTITUTION',\n",
    "    'ROBBERY',\n",
    "    'STOLEN PROPERTY',\n",
    "    'TRESPASS',\n",
    "    'VANDALISM',\n",
    "    'VEHICLE THEFT',\n",
    "    'WEAPON LAWS',\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise*: More temporal patterns. Last time we plotted the development over time (how each of the focus crimes changed over time, year-by-year). Today we'll start by looking at the developments across the months, weekdays, and across the 24 hours of the day. \n",
    ">\n",
    "> **Note:** restrict yourself to the dataset of *entire years*.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read crime data from the CSV file\n",
    "data_path = os.path.abspath(os.path.join(os.pardir, \"data\"))\n",
    "csv_name = \"Police_Department_Incident_Reports_Complete.csv\"\n",
    "df = pd.read_csv(os.path.join(data_path, csv_name))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the rows where the \"Category\" is in the focus_crimes list\n",
    "df_focus = df[df['Category'].isin(focuscrimes)]\n",
    "df_focus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * *Weekly patterns*. Basically, we'll forget about the yearly variation and just count up what happens during each weekday. [Here's what my version looks like](https://raw.githubusercontent.com/suneman/socialdata2022/main/files/weekdays.png). Some things make sense - for example `drunkenness` and the weekend. But there are some aspects that were surprising to me. Check out `prostitution` and mid-week behavior, for example!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dictonary of abbreviations for the days of the week\n",
    "days_long = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "days_abrv = ['MON', 'TUE', 'WED', 'THU', 'FRI', 'SAT', 'SUN']\n",
    "\n",
    "days = dict(zip(days_long, days_abrv))\n",
    "print(\"DICTONARY OF ABBREVIATIONS\\nFOR THE DAYS OF THE WEEK\")\n",
    "for d in days:\n",
    "    print([d, days[d]], sep=\"\\n\")\n",
    "\n",
    "# Group by category and day of the week, then count occurrences\n",
    "day_groupings = df_focus.groupby(['Category', 'Day of Week']).size().unstack().reindex(columns = days_long)\n",
    "day_groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of subplots based on number of focus crimes\n",
    "\n",
    "# Font sizes\n",
    "subtitle_fontsize = 25\n",
    "tick_fontsize = 20\n",
    "label_fontsize = 22\n",
    "\n",
    "# Colors shades\n",
    "colors = plt.cm.Blues(np.linspace(0.3, 1, len(focuscrimes)))\n",
    "\n",
    "# Dimensions of the grid\n",
    "no_cols = 3\n",
    "no_rows = int(len(focuscrimes)/no_cols)\n",
    "col_size = 12\n",
    "row_size = 6\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    no_rows,\n",
    "    no_cols,\n",
    "    figsize=(\n",
    "        col_size*no_cols,  # Hardcoded width of 8 inches per column \n",
    "        row_size*no_rows   # Hardcoded height of 4 inches per row\n",
    "    ), \n",
    "    sharex=False\n",
    ")\n",
    "\n",
    "# Flatten for easier indexing\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Find the maximum value for the y-axis\n",
    "max_val = day_groupings.max().max()\n",
    "\n",
    "# PLOTTING THE DATA\n",
    "for i, (category, count) in enumerate(day_groupings.iterrows()):\n",
    "    axs[i].bar(days_abrv, count, color=colors[i], edgecolor='black')\n",
    "    axs[i].set_ylim(\n",
    "        # count.min()*0.9, # Uncomment to show the min to max span\n",
    "        0, \n",
    "        # max_val*1.1, # Uncomment to show set the same y-axis for all plots\n",
    "        count.max()*1.1,\n",
    "    )\n",
    "    \n",
    "    axs[i].set_title(category, loc='left', fontsize = subtitle_fontsize)\n",
    "    axs[i].set_xticks(days_abrv)\n",
    "    axs[i].tick_params(axis='x', labelsize = tick_fontsize)\n",
    "    axs[i].set_ylabel('No. of incidents', fontsize = label_fontsize)\n",
    "    axs[i].set_xlabel('Weekdays', fontsize = label_fontsize)\n",
    "\n",
    "fig.tight_layout(pad = 5.0)\n",
    "plt.savefig(os.path.join(plots_path, \"W2_ex2_catperday.pdf\"), format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * *The months*. We can also check if some months are worse by counting up number of crimes in Jan, Feb, ..., Dec. Did you see any surprises there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dictonary of abbreviations for the days of the week\n",
    "months_long = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "months_abrv = ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']\n",
    "\n",
    "months = dict(zip(months_long, months_abrv))\n",
    "print(\"DICTONARY OF ABBREVIATIONS\\nFOR THE MONTHS OF THE YEAR\")\n",
    "for m in months:\n",
    "    print([m, months[m]], sep=\"\\n\")\n",
    "\n",
    "# Group by category and months, then count occurrences\n",
    "month_groupings = df_focus.groupby(['Category', 'MonthName']).size().unstack().reindex(columns = months_long)\n",
    "month_groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of subplots based on number of focus crimes\n",
    "\n",
    "# Font sizes\n",
    "subtitle_fontsize = 25\n",
    "tick_fontsize = 20\n",
    "label_fontsize = 22\n",
    "\n",
    "# Colors shades\n",
    "colors = plt.cm.BuGn(np.linspace(0.3, 1, len(focuscrimes)))\n",
    "\n",
    "# Dimensions of the grid\n",
    "no_cols = 3\n",
    "no_rows = int(len(focuscrimes)/no_cols)\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    no_rows,\n",
    "    no_cols,\n",
    "    figsize=(\n",
    "        12*no_cols,  # Hardcoded width of 8 inches per column \n",
    "        6*no_rows   # Hardcoded height of 4 inches per row\n",
    "    ), \n",
    "    sharex=False\n",
    ")\n",
    "\n",
    "# Flatten for easier indexing\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Find the maximum value for the y-axis\n",
    "max_val = month_groupings.max().max()\n",
    "\n",
    "# PLOTTING THE DATA\n",
    "for i, (category, count) in enumerate(month_groupings.iterrows()):\n",
    "    axs[i].bar(months_abrv, count, color=colors[i], edgecolor='black')\n",
    "    axs[i].set_ylim(\n",
    "        # count.min()*0.9, # Uncomment to show the min to max span\n",
    "        0, \n",
    "        # max_val*1.1, # Uncomment to show set the same y-axis for all plots\n",
    "        count.max()*1.1,\n",
    "    )\n",
    "    axs[i].set_title(category, loc='left', fontsize = subtitle_fontsize)\n",
    "    axs[i].set_xticks(months_abrv)\n",
    "    axs[i].tick_params(axis='x', labelsize = tick_fontsize)\n",
    "    axs[i].set_ylabel('No. of incidents', fontsize = label_fontsize)\n",
    "    axs[i].set_xlabel('Months', fontsize = label_fontsize)\n",
    "\n",
    "fig.tight_layout(pad = 5.0)\n",
    "plt.savefig(os.path.join(plots_path, \"W2_ex2_catpermonth.pdf\"), format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * *The 24 hour cycle*. We can also forget about weekday and simply count up the number of each crime-type that occurs in the dataset from midnight to 1am, 1am - 2am ... and so on. Again: Give me a couple of comments on what you see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dictonary of abbreviations for the days of the week\n",
    "hours_long = list(range(24))\n",
    "hours_abrv = [f\"{h:02d}\" for h in hours_long]\n",
    "\n",
    "hours = dict(zip(hours_long, hours_abrv))\n",
    "print(\"DICTONARY OF ABBREVIATIONS\\nFOR THE HOURS OF THE DAY\")\n",
    "for h in hours:\n",
    "    print([h, hours[h]], sep=\"\\n\")\n",
    "\n",
    "# Group by category and months, then count occurrences\n",
    "hour_groupings = df_focus.groupby(['Category', 'Hour']).size().unstack().reindex(columns = hours_long)\n",
    "hour_groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of subplots based on number of focus crimes\n",
    "\n",
    "# Font sizes\n",
    "subtitle_fontsize = 25\n",
    "tick_fontsize = 20\n",
    "label_fontsize = 22\n",
    "\n",
    "# Colors shades\n",
    "colors = plt.cm.YlOrBr(np.linspace(0.3, 1, len(focuscrimes)))\n",
    "\n",
    "# Dimensions of the grid\n",
    "no_cols = 3\n",
    "no_rows = int(len(focuscrimes)/no_cols)\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    no_rows,\n",
    "    no_cols,\n",
    "    figsize=(\n",
    "        12*no_cols,  # Hardcoded width of 8 inches per column \n",
    "        6*no_rows   # Hardcoded height of 4 inches per row\n",
    "    ), \n",
    "    sharex=False\n",
    ")\n",
    "\n",
    "# Flatten for easier indexing\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Find the maximum value for the y-axis\n",
    "max_val = hour_groupings.max().max()\n",
    "\n",
    "# PLOTTING THE DATA\n",
    "for i, (category, count) in enumerate(hour_groupings.iterrows()):\n",
    "    axs[i].bar(hours_abrv, count, color=colors[i], edgecolor='black')\n",
    "    axs[i].set_ylim(\n",
    "        # count.min()*0.9, # Uncomment to show the min to max span\n",
    "        0, \n",
    "        # max_val*1.1, # Uncomment to show set the same y-axis for all plots\n",
    "        count.max()*1.1,\n",
    "    )\n",
    "    axs[i].set_title(category, loc='left', fontsize = subtitle_fontsize)\n",
    "    axs[i].set_xticks(hours_abrv)\n",
    "    axs[i].tick_params(axis='x', labelsize = tick_fontsize)\n",
    "    axs[i].set_ylabel('No. of incidents', fontsize = label_fontsize)\n",
    "    axs[i].set_xlabel('Hour of day', fontsize = label_fontsize)\n",
    "\n",
    "fig.tight_layout(pad = 5.0)\n",
    "plt.savefig(os.path.join(plots_path, \"W2_ex2_catperhour.pdf\"), format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the trends, it shows that the early morning hours tend to contain less incidents compared against the rest of the day. Additionally, it seems that the incidents spike in the late afternoon to evening hours. Some of the crimes are heavily related to activities that take place in the later hours, such as drinking and prostitution. On the other side; theft and trespass suggest to be following regular opening hours of most businessess, such as grocery stores and offices.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * *Hours of the week*. But by looking at just 24 hours, we may be missing some important trends that can be modulated by week-day, so let's also check out the 168 hours of the week. So let's see the number of each crime-type Monday night from midninght to 1am, Monday night from 1am-2am - all the way to Sunday night from 11pm to midnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new x-axis that spans the full week (168 hours)\n",
    "hours_of_week = [f\"{h:02d}\" for h in range(168)]  # 0 to 167 representing hours in the week\n",
    "\n",
    "# Prepare tick labels with day names at midnights\n",
    "tick_positions = [h * 24 for h in range(7)]\n",
    "tick_labels = [days_abrv[d] for d in range(7)]\n",
    "\n",
    "# Group data: Crime count per category per hour of the week\n",
    "df_focus['HourOfWeek'] = df_focus['Day of Week'].map({day: i for i, day in enumerate(days_long)}) * 24 + df_focus['Hour']\n",
    "hour_week_groupings = df_focus.groupby(['Category', 'HourOfWeek']).size().unstack(fill_value=0).reindex(columns=range(168), fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of subplots based on number of focus crimes\n",
    "\n",
    "# Font sizes\n",
    "subtitle_fontsize = 25\n",
    "tick_fontsize = 20\n",
    "label_fontsize = 22\n",
    "\n",
    "# Dimensions of the grid\n",
    "no_cols = 3\n",
    "no_rows = int(len(focuscrimes) / no_cols)\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    no_rows,\n",
    "    no_cols,\n",
    "    figsize=(\n",
    "        12*no_cols,  # Hardcoded width of 8 inches per column \n",
    "        6*no_rows   # Hardcoded height of 4 inches per row\n",
    "    ), \n",
    "    sharex=False\n",
    ")\n",
    "\n",
    "# Flatten subplot array\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Find the maximum value for the y-axis\n",
    "max_val = hour_week_groupings.max().max()\n",
    "\n",
    "# Plot each category\n",
    "for i, (category, count) in enumerate(hour_week_groupings.iterrows()):\n",
    "    axs[i].bar(hours_of_week, count, color='silver')\n",
    "    axs[i].set_ylim(\n",
    "        # count.min()*0.9, # Uncomment to show the min to max span\n",
    "        0, \n",
    "        # max_val*1.1, # Uncomment to show set the same y-axis for all plots\n",
    "        count.max()*1.1,\n",
    "    )\n",
    "    axs[i].set_title(category, loc='left', fontsize=subtitle_fontsize)\n",
    "\n",
    "    # Set x-ticks at midnight for each day\n",
    "    axs[i].set_xticks(tick_positions)\n",
    "    axs[i].set_xticklabels(tick_labels, fontsize=tick_fontsize)\n",
    "    axs[i].tick_params(axis='x', which='both', labelrotation=45)\n",
    "    axs[i].set_ylabel('No. of incidents', fontsize = label_fontsize)\n",
    "    axs[i].set_xlabel('Hour of day spaning weekdays', fontsize = label_fontsize)\n",
    "\n",
    "    # # Update color for each midnight to show the start of a new day\n",
    "    for idx, bar in enumerate(axs[i].patches):\n",
    "        if idx % 24 == 0:\n",
    "            bar.set_color(\"red\")\n",
    "        bar.set_edgecolor('black')\n",
    "\n",
    "fig.tight_layout(pad=5.0)\n",
    "plt.savefig(os.path.join(plots_path, \"W2_ex2_catperhourweek.pdf\"), format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fun little thing**: To further make the point of the exercise in Part 1, check out this video on youtube https://www.youtube.com/watch?v=DbJyPELmhJc  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Creating nice plots\n",
    "> *Exercise:* Nice plots. \n",
    ">\n",
    "> * Create a list of 10 rules for nice plots based on the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rule 1 - _Subplots_:** \n",
    "* If we need to show multiple (similar) plots regarding the same problem or data, we should strongly consider using subplots. \n",
    "\n",
    "**Rule 2 - _Crop tightly_:** \n",
    "* When presenting plots in a paper, it is important to zoom it tighly, reducing redundant spacing. \n",
    "\n",
    "**Rule 3 - _Labels_:** \n",
    "* \"Plots with no labels will get you fired!\". It is important to show the reader what the data is, i.e. think of it as providing neccessary metadata for context. \n",
    "\n",
    "**Rule 4 - _Axes 1 Meaningful values_:** \n",
    "* The axes tick intervals and values should intuitivly make sense, e.g. intervals of 7 for days to associate with weeks.\n",
    "\n",
    "**Rule 5 - _Axes 2 Truthful limits_:** \n",
    "* The axes should not leave out information in order to show trends, e.g. zooming in on the y-axis in order to show differences, when these in reality are miniscule.\n",
    "\n",
    "**Rule 6 - _Axes 3 Subplot limits_:** \n",
    "* The axes limits of subplots should not differ if compared direcctly, e.g. when working with quantities and not concentrations.\n",
    "\n",
    "**Rule 7 - _Data to Ink ratio 1_:** \n",
    "* The amount of the total ink used to present data should be maximized; _Keep it simple and straight to the point_! \n",
    "\n",
    "**Rule 8 - _Data to Ink ratio 2_:** \n",
    "* Redundent visualisation should be removed to highlight the important parts of the data; _Less is more_!\n",
    "\n",
    "**Rule 9 - _Fonts_:** \n",
    "* Changing the font is just an example of showing attention to detail. Be thoughtful with how you present your findings.\n",
    "\n",
    "**Rule 10 - _Captions_:** \n",
    "* Captions are very useful to convey information alongside visualizations. A good caption will explain each component of the graphic and clearing up any potential confusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Exploring other types of plots for temporal data\n",
    "\n",
    "We continue our mini-break from barcharts by exploring more ways to plot temporal data.\n",
    "\n",
    "> *Exercise:* Other cool ways to plot temporal data. I am going to introduce three different plot-types. Then your job is to choose a part of the crime-data that you care about - and plot it using these new ways of visualizing data. <font color=\"grey\">For this one, you may use the full power of your LLM! Yes, I'm serious. Go nuts and see if you can get these plots going by fully relying on code generated by your LLM</font>. In case you want to use the *old ways* (that's of course always OK), I've included links and tips. \n",
    ">\n",
    ">I recommend that you choose a different part of the crime-data for each plot-type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder of what the data looks like\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Calendar plots. Get started on calendar plots **[here](https://calplot.readthedocs.io/en/latest/)**. There are other packages for plotting these, those are also OK to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install calplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Date column by combining Year, Month, and Day\n",
    "df['Date'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
    "\n",
    "# Count incidents per date\n",
    "events = df['Date'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create colormap from green to red (few to many incidents)\n",
    "cmap_orig = plt.colormaps['RdYlGn']\n",
    "cmap_reversed = cmap_orig.reversed()\n",
    "\n",
    "# Plot the calendar heatmap\n",
    "calplot.calplot(\n",
    "    data=events, \n",
    "    cmap=cmap_reversed, \n",
    "    fillcolor=\"Blue\",\n",
    "    colorbar=True,\n",
    "    yearlabel_kws={\n",
    "        'fontname':'sans-serif'\n",
    "    }\n",
    ")\n",
    "\n",
    "# show plot ot supress the return value of the calplot function\n",
    "plt.savefig(os.path.join(plots_path, \"W2_ex4_calplot.pdf\"), format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * [Polar bar chart](https://user-images.githubusercontent.com/12328192/89272649-be76e200-d63e-11ea-97ad-fd1ba5831c89.png). Here I want you to plot a 24-hour pattern of some sort -- those work really well in radial plots (another name for polar plots) because the day  wraps around on itself. You can also try plotting data with patterns from the 168 hours of the week. There's not one super-awesome solution here, you can try using [pure matplotlib](https://matplotlib.org/stable/gallery/pie_and_polar_charts/polar_bar.html) ... [some examples here](https://www.python-graph-gallery.com/circular-barplot/) or via [plotly](https://plotly.com/python/polar-chart/) (scroll down a bit for the polar barchart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count incidents per date\n",
    "incident_per_hour = df['Hour'].value_counts().sort_index()\n",
    "incident_per_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set figure size\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# plot polar axis\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "# remove grid\n",
    "plt.axis('off')\n",
    "\n",
    "# Set the coordinates limits\n",
    "upperLimit = 100\n",
    "lowerLimit = 25\n",
    "\n",
    "# Normalize the data for easier plotting\n",
    "v = incident_per_hour.values\n",
    "v_norm = (v - v.min())/(v.max() - v.min())\n",
    "\n",
    "# Compute the heights (radius) of the bars from the normalized data\n",
    "heights = upperLimit*v_norm + lowerLimit\n",
    "\n",
    "# Compute the width of each bar. In total we have 2*Pi = 360Â°\n",
    "width = 2*np.pi / len(incident_per_hour.index)\n",
    "\n",
    "# Compute the angle each bar is centered on:\n",
    "indexes = list(reversed(range(1, len(incident_per_hour.index)+1)))\n",
    "angles = [element * width for element in indexes]\n",
    "angles = [angle + np.pi / 2 for angle in angles]\n",
    "\n",
    "# Draw bars\n",
    "bars = ax.bar(\n",
    "    x=angles,\n",
    "    height=heights, \n",
    "    width=width, \n",
    "    bottom=lowerLimit,\n",
    "    linewidth=3, \n",
    "    edgecolor=\"white\",\n",
    "    color=\"deepskyblue\",\n",
    ")\n",
    "\n",
    "# little space between the bar and the label\n",
    "labelPadding = 5.0\n",
    "\n",
    "# Colors shades\n",
    "colors_ori = plt.cm.coolwarm(np.linspace(0.1, 1, 13)).tolist()\n",
    "colors_rev = list(reversed(colors_ori[:-1]))\n",
    "colors = colors_ori + colors_rev\n",
    "c = 0\n",
    "\n",
    "# Add labels\n",
    "for bar, angle, height, label in zip(bars,angles, heights, incident_per_hour.index):\n",
    "\n",
    "    # Coloring the bars using the colormap\n",
    "    bar.set_color(colors[c])\n",
    "    c += 1\n",
    "\n",
    "    # Labels are rotated. Rotation must be specified in degrees :(\n",
    "    rotation = np.rad2deg(angle)\n",
    "\n",
    "    # Flip some labels upside down\n",
    "    alignment = \"\"\n",
    "    if angle >= np.pi/2 and angle < 3*np.pi/2:\n",
    "        alignment = \"right\"\n",
    "        rotation = rotation + 180\n",
    "    else: \n",
    "        alignment = \"left\"\n",
    "\n",
    "    # Finally add the labels\n",
    "    # print(height)\n",
    "    ax.text(\n",
    "        x=angle, \n",
    "        y=lowerLimit + height + labelPadding, \n",
    "        s=f\"{label:02d}:00\", \n",
    "        ha=alignment, \n",
    "        va='center', \n",
    "        rotation=rotation, \n",
    "        rotation_mode=\"anchor\"\n",
    "    ) \n",
    "\n",
    "# CODE FOR CREATING COLORBAR (IN THIS CASE WRONFULLY USED)\n",
    "# Create a ScalarMappable to map values to colors\n",
    "sm = plt.cm.ScalarMappable(cmap='coolwarm', norm=plt.Normalize(vmin=v.min(), vmax=v.max()))\n",
    "sm.set_array([])  # this is needed for the colorbar to display correctly\n",
    "\n",
    "# Add the colorbar\n",
    "fig = plt.gcf()  # Get the current figure\n",
    "cbar = fig.colorbar(sm, ax=ax, orientation='horizontal', pad=0.1, aspect=50)\n",
    "cbar.set_label('Total number of reported incidents by hour of day', fontsize=12, fontname='monospace')\n",
    "# ticks = [v.min()] + list(range(40000, v.max(), 10000)) + [v.max()]\n",
    "ticks = []\n",
    "\n",
    "cbar.set_ticks(ticks)\n",
    "# cbar.ax.set_xticklabels(\n",
    "#     ticks, \n",
    "#     rotation=45, \n",
    "#     fontname='monospace'\n",
    "# )\n",
    "\n",
    "# Add \"min\" and \"max\" labels at each end\n",
    "cbar.ax.text(v.min(), 2, 'Midnight', ha='left', va='center', fontsize=12, fontname='monospace')\n",
    "cbar.ax.text(v.max(), 2, 'Noon', ha='right', va='center', fontsize=12, fontname='monospace')\n",
    "\n",
    "plt.savefig(os.path.join(plots_path, \"W2_ex4_polarbarplot.pdf\"), format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Time series. Time series is a key functionality of `Pandas`, so here I simply recommend starting by searching your favorite search engine for something like `time series` `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going through the [GeeksforGeeks Time Series Manipulation](https://www.geeksforgeeks.org/pandas-time-series-manipulation/), we learned how Time series are used to organize data by timestamps.\n",
    "\n",
    "The main objectives of _**Time Series Analysis**_ are:\n",
    "\n",
    "- Create a series of date\n",
    "- Work with data timestamp\n",
    "- Convert string data to timestamp\n",
    "- Slicing of data using timestamp\n",
    "- Resample your time series for different time period aggregates/summary statistics\n",
    "- Working with missing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Reflection (<font color=\"grey\">no LLM help here</font>): What did you learn from using LLM's to simply solve everything in this exercise (in contrast to the previous ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we primarily used the provided - or found documentation for the matplotlib plots. Some polishing was provided by ChatGPT to speed up the process, but it was solely used for cosmetic purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Back to visualizing patterns in the data\n",
    "\n",
    "The next thing we'll be looking into is how crimes break down across the 10 districts in San Francisco. <font color=\"grey\">See if you can do this on your own - the calculations are fairly simple. If you get stuck, you may ask your LLM for advice, but don't use it to simply generate the plots. Use it as a help to get hints at how to solve the things you can't figure out on your own.</font>\n",
    "\n",
    "> *Exercise*: The types of crime and how they take place across San Francisco's police districts.\n",
    ">  \n",
    ">  * So now we'll be combining information about `PdDistrict` and `Category` to explore differences between SF's neighborhoods. First, simply list the names of SF's 10 police districts.\n",
    ">  * Which has the most crimes? Which has the most focus crimes?\n",
    ">  * Next, we want to generate a slightly more complicated graphic. I'm interested to know if there are certain crimes that happen much more in certain neighborhoods than what's typical. Below I describe how to get that plot going:\n",
    ">    - First, we need to calculate the relative probabilities of seeing each type of crime in the dataset as a whole. That's simply a normalized version of [this plot](https://raw.githubusercontent.com/suneman/socialdata2022/main/files/CrimeOccurrencesByCategory.png). Let's call it `P(crime)`.\n",
    ">    - Next, we calculate that same probability distribution _but for each PD district_, let's call that `P(crime|district)`.\n",
    ">    - Now we look at the ratio `P(crime|district)/P(crime)`. That ratio is equal to 1 if the crime occurs at the same level within a district as in the city as a whole. If it's greater than one, it means that the crime occurs _more frequently_ within that district. If it's smaller than one, it means that the crime is _rarer within the district in question_ than in the city as a whole.\n",
    ">    - For each district plot these ratios for the 14 focus crimes. My plot (based on 2003-2018 data) looks like this\n",
    "> ![Histograms](https://raw.githubusercontent.com/suneman/socialdata2022/main/files/conditional.png \"histograms\")\n",
    ">    - Comment on the top crimes in _Tenderloin_, _Mission_, and _Richmond_. Does this fit with the impression you get of these neighborhoods on Wikipedia <font color=\"grey\">(or from your LLM's description of those neighborhoods)</font>?\n",
    ">    - What neighborhood would you prefer to live in? Explain why?\n",
    ">    - <font color=\"grey\">BONUS QUESTION. Can you get your LLM to discuss the graphic you created? (E.g. by showing it the image) Did it notice anything you didn't?</font>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Again, there are detailed LLM hints in gray in the text itself. The key advice is to always see if you can do this on your own - the calculations are fairly simple. Then ask an LLM for advice if you get stuck. Never use it to simply generate the plots. Use it as a help to get hints at how to solve the things you can't figure out on your own - that's how you get familar with how matplotlib actually works.\n",
    "</div>\n",
    "\n",
    "**Comment**. Notice how much awesome data science (i.e. learning about interesting real-world crime patterns) we can get out by simply counting and plotting (and looking at ratios). Pretty great, right? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02806",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
