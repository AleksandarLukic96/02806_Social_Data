{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas display options to show all columns for .head command\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)        # Auto-detect the display width\n",
    "pd.set_option('display.max_colwidth', None) # Show full content of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\erago\\\\Documents\\\\GitHub\\\\02806_Social_Data\\\\files'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get path of data directory\n",
    "files_path = os.path.abspath(os.path.join(os.pardir, \"files\"))\n",
    "files_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create paths to the datasets\n",
    "data_1_path = os.path.join(files_path, \"data1.tsv\")\n",
    "data_2_path = os.path.join(files_path, \"data2.tsv\")\n",
    "data_3_path = os.path.join(files_path, \"data3.tsv\")\n",
    "data_4_path = os.path.join(files_path, \"data4.tsv\") \n",
    "\n",
    "# Load the datasets from the files folder\n",
    "df_1 = pd.read_csv(data_1_path,sep='\\t', header=None)\n",
    "df_2 = pd.read_csv(data_2_path,sep='\\t', header=None)\n",
    "df_3 = pd.read_csv(data_3_path,sep='\\t', header=None)\n",
    "df_4 = pd.read_csv(data_4_path,sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: A little visualization exercise\n",
    "\n",
    "\n",
    "> *Exercise*: \n",
    "> \n",
    "> * Using the `numpy` function `mean`, calculate the mean of both $x$-values and $y$-values for each dataset. \n",
    ">      * Use python string formatting to print precisely two decimal places of these results to the output cell. Check out [this _stackoverflow_ page](http://stackoverflow.com/questions/8885663/how-to-format-a-floating-number-to-fixed-width-in-python) for help with the string formatting. <font color='grey'>You may also ask an LLM about the string formatting, but make sure it doesn't just give you the answer: Ask how string formatting works, get some examples, and solve your own problem based on that.</font>\n",
    "> * Now calculate the variance for all of the various sets of $x$- and $y$-values, by using the `numpy` function `var`. <font color='grey'>You should be able to do this without any help from LLMs.</font> Print it to three decimal places.\n",
    "> * Use `numpy` to calculate the [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient) between $x$- and $y$-values for all four data sets (also print to three decimal places).\n",
    "> * The next step is use _linear regression_ to fit a straight line $f(x) = a x + b$ through each dataset and report $a$ and $b$ (to two decimal places). An easy way to fit a straight line in Python is using `scipy`'s `linregress`. It works like this\n",
    "> ```\n",
    "> from scipy import stats\n",
    "> a, b, r_value, p_value, std_err = stats.linregress(x,y)\n",
    ">```\n",
    "> * Comment on the results from the previous steps. What do you observe? \n",
    "> * Finally, it's time to plot the four datasets using `matplotlib.pyplot`. Use a two-by-two [`subplot`](http://matplotlib.org/examples/pylab_examples/subplot_demo.html) to put all of the plots nicely in a grid and use the same $x$ and $y$ range for all four plots. And include the linear fit in all four plots. (To get a sense of what I think the plot should look like, you can take a look at my version [here](https://raw.githubusercontent.com/suneman/socialdata2023/main/files/anscombe.png).) <font color='grey'>For this sub-exercise, try to write the code based on the example I link to. If you get stuck, you may use your LLM to figure it out. But don't ask it for the solution, ask for help with figuring out how to use the functions!</font>\n",
    "> * Explain - in your own words - what you think my point with this exercise is (see below for tips on this). <font color='grey'>Again, try to write down your own thoughts first. Then you can ask your LLM for help after that</font>.\n",
    "> * <font color='grey'>What did you get out of asking the LLM about the previous sub-question? How did you even go about asking the LLM about the point of the entire set of questions? Reflect on whether or not the LLM helped you get smarter?</font>\n",
    "\n",
    "\n",
    "Get more insight in the ideas behind this exercise by reading [here](https://en.wikipedia.org/wiki/Anscombe%27s_quartet). Here you can also get an explanation of why the datasets are actually famous - I mean they have their own Wikipedia page!! <font color='grey'>After reading the wikipedia page, can you learn something from a LLM that's not in the wikipedia page?</font>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "All of the important LLM comments are in gray above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use mean on each data set to get x and y mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(9.0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_x_1 = np.mean(df_1.iloc[:, 0])\n",
    "mean_x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(7.500909090909093)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_y_1 = np.mean(df_1.iloc[:, 1])\n",
    "mean_y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     10\n",
       "1      8\n",
       "2     13\n",
       "3      9\n",
       "4     11\n",
       "5     14\n",
       "6      6\n",
       "7      4\n",
       "8     12\n",
       "9      7\n",
       "10     5\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.iloc[:, 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
